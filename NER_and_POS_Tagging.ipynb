{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER and POS-Tagging.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZZZ8jQXTHTyC"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roh15/Lok-Sabha-Dataset/blob/main/NER_and_POS_Tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3GXzZ5bRQ8a"
      },
      "source": [
        "# **Stanza for English NER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzo8pyVzU13E"
      },
      "source": [
        "!pip install stanza\n",
        "!pip install stanza-batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjby4p2nXGLP"
      },
      "source": [
        "import stanza\n",
        "import pandas as pd\n",
        "from stanza.models.common.doc import Document\n",
        "from typing import List\n",
        "from stanza_batch import batch\n",
        "stanza.download(lang='en', processors='tokenize,ner')\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6ME_-fzFz6d"
      },
      "source": [
        "# not done: 12, 17, 38, 39\n",
        "\n",
        "ls_num = 16\n",
        "li = [181]\n",
        "for file in li:\n",
        "  stanza_documents: List[Document] = []\n",
        "  text_filename = '/content/drive/MyDrive/Lok Sabha Data/textfiles/' + str(ls_num) + '/' + str(file) + '.csv'\n",
        "  textfile = pd.read_csv(text_filename, lineterminator='\\n', names=['index','speech'])\n",
        "  if textfile['speech'].isnull().all():\n",
        "    print(str(ls_num) + '-' + str(file) + ' is empty!')\n",
        "  else:\n",
        "    speech = [x for x in textfile['speech'].values if x]\n",
        "    for document in batch(speech, nlp, batch_size=32):\n",
        "      stanza_documents.append(document)\n",
        "    ner = []\n",
        "    for i in range(len(stanza_documents)):\n",
        "      for j in range(len(stanza_documents[i].sentences)):\n",
        "        for k in range(len(stanza_documents[i].sentences[j].ents)):\n",
        "          ner.append([i-1, j, k, stanza_documents[i].sentences[j].ents[k].text, stanza_documents[i].sentences[j].ents[k].type]) \n",
        "    NER = pd.DataFrame(ner, columns=['speech#', 'sent#', 'ent#', 'entity_text', 'entity_type'])\n",
        "    NER_filename = '/content/drive/MyDrive/Lok Sabha Data/NER/' + str(ls_num) + '/' + str(file) + '_NER.csv'\n",
        "    NER.to_csv(NER_filename)\n",
        "    print(str(ls_num) + '-' + str(file) + ' NER done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4vylaEsNl3S"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFE2aebARBng"
      },
      "source": [
        "# **Stanza for Hindi POS-tagging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIa0IKSbRdHI"
      },
      "source": [
        "!pip install stanza\n",
        "!pip install stanza-batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnVOgB44RdHM"
      },
      "source": [
        "import stanza\n",
        "import pandas as pd\n",
        "from stanza.models.common.doc import Document\n",
        "from typing import List\n",
        "from stanza_batch import batch\n",
        "stanza.download(lang='hi', processors='tokenize,pos')\n",
        "nlp = stanza.Pipeline(lang='hi', processors='tokenize,pos')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0ELag21RARy"
      },
      "source": [
        "ls_num = 17\n",
        "li = list(range(112,1,-1))\n",
        "for file in li:\n",
        "  stanza_documents: List[Document] = []\n",
        "  text_filename = '/content/drive/MyDrive/Lok Sabha Data/textfiles/' + str(ls_num) + '/' + str(file) + '.csv'\n",
        "  textfile = pd.read_csv(text_filename, lineterminator='\\n', names=['index','speech'])\n",
        "  if textfile['speech'].isnull().all():\n",
        "    print(str(ls_num) + '-' + str(file) + ' is empty!')\n",
        "  else:\n",
        "    speech = [x for x in textfile['speech'].values if x]\n",
        "    for document in batch(speech, nlp, batch_size=32):\n",
        "      stanza_documents.append(document)\n",
        "    pos = []\n",
        "    for i in range(len(stanza_documents)):\n",
        "      for j in range(len(stanza_documents[i].sentences)):\n",
        "        for k in range(len(stanza_documents[i].sentences[j].words)):\n",
        "          pos.append([i-1, j, k, stanza_documents[i].sentences[j].words[k].text, stanza_documents[i].sentences[j].words[k].upos, stanza_documents[i].sentences[j].words[k].xpos, stanza_documents[i].sentences[j].words[k].feats]) \n",
        "    POS = pd.DataFrame(pos, columns=['speech#', 'sent#', 'word#', 'word', 'word_upos', 'word_xpos', 'word_feats'])\n",
        "    POS_filename = '/content/drive/MyDrive/Lok Sabha Data/POS/' + str(ls_num) + '/' + str(file) + '_POS.csv'\n",
        "    POS.to_csv(POS_filename)\n",
        "    print(str(ls_num) + '-' + str(file) + ' POS done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqkl1Ei1S7P6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZZ8jQXTHTyC"
      },
      "source": [
        "# **DeepPavlov for Hindi**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN1g5MBmTPId"
      },
      "source": [
        "This was a failed attempt at performing NER on the Hindi text. The model could not tokenize Hindi words and therefore could not perform NER."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb4of4xP_Aig"
      },
      "source": [
        "pip install deeppavlov"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f37Dwbgba-E"
      },
      "source": [
        "!python -m deeppavlov install ner_ontonotes_bert_mult\n",
        "!python -m deeppavlov interact ner_ontonotes_bert_mult [-d]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF8EQt-5-M5G"
      },
      "source": [
        "from deeppavlov import configs, build_model\n",
        "\n",
        "ner_model = build_model(configs.ner.ner_ontonotes_bert_mult, download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyOzlQ0CCJI1"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5xwPsZ1_w6S"
      },
      "source": [
        "ls_num = 17\n",
        "li = [1]\n",
        "for file in li:\n",
        "  text_filename = '/content/drive/MyDrive/Lok Sabha Data/textfiles/' + str(ls_num) + '/' + str(file) + '.csv'\n",
        "  textfile = pd.read_csv(text_filename, lineterminator='\\n', names=['index','speech'])\n",
        "  if textfile['speech'].isnull().all():\n",
        "    print(str(ls_num) + '-' + str(file) + ' is empty!')\n",
        "  else:\n",
        "    speech = [x for x in textfile['speech'].values if x]\n",
        "    ner = ner_model([speech[7]])\n",
        "    NER = pd.DataFrame(ner, columns=['ner'])\n",
        "    # NER_filename = '/content/drive/MyDrive/Lok Sabha Data/NER/' + str(ls_num) + '/' + str(file) + '_NER.csv'\n",
        "    # NER.to_csv(NER_filename)\n",
        "    # print(str(ls_num) + '-' + str(file) + ' NER done!')\n",
        "NER.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0drctL-OCEVl"
      },
      "source": [
        "speech[7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhAQmisEEUlD"
      },
      "source": [
        "ner_model([speech[7]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYbdClqpFGkE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}